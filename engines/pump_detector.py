#!/usr/bin/env python3
# SPDX-License-Identifier: BUSL-1.1
"""
pump_detector.py - SaferTrade Pump Detection Engine

Detects coordinated buying campaigns, artificial price pumps, and market manipulation
schemes. Monitors trading volume, social sentiment, and price movements to identify
potential pump and dump schemes in real-time.

Core functionality:
- Monitors unusual volume spikes and price movements from REAL DEX data
- Correlates social media sentiment with trading activity
- Detects coordinated buying patterns across exchanges
- Identifies potential market manipulation schemes

FIXED: Removed all fake data generators. Now uses:
- DexScreener API for real token data
- CoinGecko API for trending tokens
- Real blockchain transaction data
"""

import asyncio
import json
import logging
import os
import sqlite3
import sys
import time
from datetime import datetime, timedelta, timezone
from typing import Dict, List, Optional, Tuple

import numpy as np
import redis
import requests
from web3 import Web3

# Ensure project root is on sys.path so `shared.*` imports resolve reliably
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), "..")))
from shared.chains import get_web3_for_chain
from shared.database_config import connect_db, connect_main, get_main_db_path
from shared.env import load_env
from shared.logging_setup import setup_logging
from shared.paths import ROOT_DIR

# Import existing components for integration
sys.path.insert(0, os.path.join(os.path.dirname(__file__)))
try:
    from reddit_monitor import RedditMonitorV2

    reddit_available = True
except Exception:
    reddit_available = False

# Note: sentiment_collector was removed (merged into social_intelligence_aggregator)
sentiment_available = False
SentimentCollector = None  # Stub for type checker


class PumpDetector:
    def __init__(self):
        # Initialize Redis connection for real-time data streaming
        self.redis_client = redis.Redis(
            host=os.getenv("REDIS_HOST", "localhost"),
            port=int(os.getenv("REDIS_PORT", 6379)),
            decode_responses=True,
        )

        # Initialize database connection
        self.db_path = get_main_db_path()

        # Initialize supporting engines (if available)
        self.reddit_monitor = RedditMonitorV2() if reddit_available else None
        self.sentiment_collector = SentimentCollector() if sentiment_available else None

        # Initialize Web3 via chain manager (prefers local Erigon with fallback)
        try:
            self.w3 = get_web3_for_chain("ethereum")
            try:
                provider_uri = getattr(
                    getattr(self.w3, "provider", None), "endpoint_uri", None
                )
                self.provider_info = (
                    str(provider_uri)
                    if provider_uri
                    else self.w3.provider.__class__.__name__
                )
            except Exception:
                self.provider_info = (
                    self.w3.provider.__class__.__name__
                    if getattr(self.w3, "provider", None)
                    else "unknown"
                )
        except Exception as e:
            # Keep object usable even if provider init fails; downstream code should guard
            self.w3 = Web3()
            self.provider_info = f"uninitialized ({e})"

        # Setup logging
        setup_logging("pump_detector", ROOT_DIR)
        self.logger = logging.getLogger("pump_detector")

        # API endpoints for REAL data - Local Erigon preferred, external APIs as fallback
        self.dexscreener_api = "https://api.dexscreener.com/latest"  # Kept as fallback
        self.coingecko_api = "https://api.coingecko.com/api/v3"  # Kept as fallback

        # Pump detection algorithms and thresholds
        self.detection_params = self._initialize_detection_params()

        # Initialize database table for pump detection
        self._init_database()

        self.logger.info(
            "‚úÖ Pump Detector initialized (REAL DATA MODE) | web3 provider=%s",
            getattr(self, "provider_info", "unknown"),
        )

    def health(self) -> Dict:
        """Minimal health check with real connectivity; no side effects."""
        # DB table presence
        db_ok = False
        try:
            conn = connect_main(timeout=20.0)
            cur = conn.cursor()
            cur.execute(
                "SELECT name FROM sqlite_master WHERE type='table' AND name='pump_detection'"
            )
            db_ok = cur.fetchone() is not None
            conn.close()
        except Exception:
            db_ok = False

        # Redis
        redis_ok = False
        try:
            redis_ok = bool(self.redis_client.ping())
        except Exception:
            redis_ok = False

        # Web3
        web3_ok = False
        try:
            web3_ok = bool(self.w3 and self.w3.is_connected())
        except Exception:
            web3_ok = False

        # Check local node first (main connectivity indicator)
        dexscreener_ok = False
        coingecko_ok = False

        # Local node connectivity is prioritized over external APIs
        local_node_ok = web3_ok

        # Only check external APIs as fallback indicators
        try:
            r = requests.get(f"{self.dexscreener_api}/dex/search?q=ETH", timeout=3)
            dexscreener_ok = r.status_code == 200
        except Exception:
            dexscreener_ok = (
                False  # External API not available, but that's OK - we prefer local
            )
        try:
            r = requests.get(f"{self.coingecko_api}/search/trending", timeout=3)
            coingecko_ok = r.status_code == 200
        except Exception:
            coingecko_ok = (
                False  # External API not available, but that's OK - we prefer local
            )

        return {
            "service": "pump_detector",
            "database_connected": db_ok,
            "redis_connected": redis_ok,
            "web3_connected": web3_ok,  # This connects to local Erigon when available
            "local_node_connected": local_node_ok,  # Our preferred data source
            "dexscreener_connected": dexscreener_ok,  # Fallback
            "coingecko_connected": coingecko_ok,  # Fallback
        }

    def _init_database(self):
        """Initialize database table for pump detection data"""
        try:
            # Use hardened shared connection helper (WAL + busy timeout)
            conn = connect_main(timeout=120)
            cursor = conn.cursor()

            # Create table for pump detection
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS pump_detection (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    timestamp DATETIME DEFAULT CURRENT_TIMESTAMP,
                    token_address TEXT,
                    token_symbol TEXT,
                    pump_score REAL,
                    risk_level TEXT,
                    detected_patterns TEXT,
                    volume_spike_ratio REAL,
                    price_momentum REAL,
                    social_signals TEXT
                )
            """)

            conn.commit()
            conn.close()
            self.logger.info("Pump detection database initialized")
        except Exception as e:
            self.logger.error(f"Error initializing pump detection database: {e}")

    def _initialize_detection_params(self) -> Dict:
        """Initialize pump detection parameters and thresholds"""
        return {
            # Volume thresholds
            "volume_spike_threshold": 5.0,  # 5x normal volume
            "volume_acceleration_threshold": 2.0,  # 2x acceleration
            "volume_concentration_threshold": 0.7,  # 70% of volume from few addresses
            # Price thresholds
            "price_rapid_increase_threshold": 0.3,  # 30% in short time
            "price_momentum_threshold": 0.5,  # High momentum score
            "price_volatility_threshold": 0.4,  # High volatility
            # Time windows (in minutes)
            "short_window": 5,  # 5 minutes for rapid detection
            "medium_window": 15,  # 15 minutes for sustained moves
            "long_window": 60,  # 60 minutes for trend confirmation
            # Correlation thresholds
            "social_price_correlation_threshold": 0.6,  # 60% correlation
            "volume_price_correlation_threshold": 0.7,  # 70% correlation
        }

    def get_trending_tokens(self, limit: int = 20) -> List[Dict]:
        """
        Get trending tokens from local Erigon node using social media signals and on-chain data,
        with CoinGecko API as fallback (REAL DATA)
        """
        try:
            # First try to get trending tokens from local analysis (social media, on-chain signals)
            local_trending = self.get_trending_tokens_from_local(limit=limit)
            if local_trending and len(local_trending) > 0:
                self.logger.info(
                    f"Found {len(local_trending)} trending tokens from local analysis"
                )
                return local_trending

            # If local analysis doesn't yield results, fall back to external API
            self.logger.info("Fetching trending tokens from CoinGecko (fallback)...")

            # Use trending search endpoint (no rate limit)
            url = f"{self.coingecko_api}/search/trending"

            response = requests.get(url, timeout=10)
            response.raise_for_status()
            data = response.json()

            trending_tokens = []
            coins = data.get("coins", [])[:limit]

            for coin_data in coins:
                coin = coin_data.get("item", {})

                trending_tokens.append(
                    {
                        "symbol": coin.get("symbol", "").upper(),
                        "name": coin.get("name", ""),
                        "address": coin.get("id", ""),  # CoinGecko ID
                        "price_btc": coin.get("price_btc", 0),
                        "market_cap_rank": coin.get("market_cap_rank", 0),
                        "score": coin.get("score", 0),
                        "source": "coingecko",  # Indicate this came from external API
                    }
                )

            self.logger.info(
                f"Found {len(trending_tokens)} trending tokens from CoinGecko"
            )
            return trending_tokens

        except Exception as e:
            self.logger.error(f"Error fetching trending tokens: {e}")
            return []

    def get_trending_tokens_from_local(self, limit: int = 20) -> List[Dict]:
        """
        Get trending tokens based on local analysis:
        - Recent large transactions
        - Social media mentions from reddit_monitor
        - On-chain activity patterns
        """
        try:
            trending_tokens = []

            # If we have access to social media monitoring (e.g., reddit_monitor integration)
            if self.sentiment_collector:
                # Get tokens mentioned frequently in social media recently
                recent_tokens = self.sentiment_collector.get_recent_tokens(limit=limit)
                for token in recent_tokens:
                    trending_tokens.append(
                        {
                            "symbol": token.get("symbol", "").upper(),
                            "name": token.get("name", ""),
                            "address": token.get("address", ""),
                            "price_btc": 0,  # Placeholder - would come from local price service
                            "market_cap_rank": 0,
                            "score": token.get("sentiment_score", 0),
                            "source": "local_social",
                        }
                    )

            # If we have transaction analysis data from local node
            # This would typically come from analyzing recent blocks for popular tokens
            # For now, we can analyze recent transactions for commonly traded tokens
            if self.w3 and self.w3.is_connected():
                # We could implement logic to analyze recent blocks for popular tokens
                # For example, by tracking popular token contracts in recent transactions
                pass

            # Limit results
            return trending_tokens[:limit]

        except Exception as e:
            self.logger.error(f"Error in local trending token analysis: {e}")
            return []

    def get_dexscreener_pairs(
        self, search_terms: List[str] = None, limit: int = 50
    ) -> List[Dict]:
        """
        Get latest trading pairs from local Uniswap/Sushiswap contracts (REAL DATA)
        Uses DexScreener as fallback to supplement data
        """
        try:
            if not search_terms:
                # Default high-volume tokens to search
                search_terms = ["PEPE", "SHIB", "DOGE", "FLOKI", "BONK"]

            # First, try to get pairs from local node via DEX contracts
            local_pairs = self.get_local_dex_pairs(search_terms, limit)

            if local_pairs and len(local_pairs) > 0:
                self.logger.info(
                    f"Found {len(local_pairs)} DEX pairs from local analysis"
                )
                return local_pairs

            # If local data is insufficient, fall back to external API
            self.logger.info("Fetching DEX pairs from DexScreener (fallback)...")

            all_pairs = []

            for term in search_terms[:3]:  # Limit searches to avoid rate limiting
                try:
                    url = f"{self.dexscreener_api}/dex/search?q={term}"

                    response = requests.get(url, timeout=10)
                    response.raise_for_status()
                    data = response.json()

                    pairs = data.get("pairs", [])

                    for pair in pairs[: limit // len(search_terms)]:
                        token_address = pair.get("baseToken", {}).get("address", "")
                        if not token_address or not Web3.is_address(token_address):
                            continue

                        # Only include pairs with significant volume
                        volume_24h = float(pair.get("volume", {}).get("h24", 0) or 0)
                        if volume_24h < 10000:  # Minimum $10k volume
                            continue

                        all_pairs.append(
                            {
                                "symbol": pair.get("baseToken", {}).get("symbol", ""),
                                "address": Web3.to_checksum_address(token_address),
                                "pair_address": pair.get("pairAddress", ""),
                                "price_usd": float(pair.get("priceUsd", 0) or 0),
                                "volume_24h": volume_24h,
                                "price_change_24h": float(
                                    pair.get("priceChange", {}).get("h24", 0) or 0
                                ),
                                "liquidity_usd": float(
                                    pair.get("liquidity", {}).get("usd", 0) or 0
                                ),
                                "txns_24h": pair.get("txns", {}).get("h24", {}),
                                "dex": pair.get("dexId", ""),
                                "chain": pair.get("chainId", ""),
                                "source": "dexscreener",  # Indicate this came from external API
                            }
                        )

                    # Rate limiting - be nice to API
                    time.sleep(1)

                except Exception as e:
                    self.logger.error(f"Error searching for {term}: {e}")
                    continue

            self.logger.info(f"Found {len(all_pairs)} valid DEX pairs from DexScreener")
            return all_pairs

        except Exception as e:
            self.logger.error(f"Error fetching DEX pairs: {e}")
            return []

    def get_local_dex_pairs(
        self, search_terms: List[str] = None, limit: int = 50
    ) -> List[Dict]:
        """
        Get DEX pairs from local Erigon node by querying Uniswap/Sushiswap contracts directly
        """
        try:
            if not self.w3 or not self.w3.is_connected():
                return []  # Local node not available, use external fallback

            # Common DEX router addresses that we'll query
            common_pairs = [
                # Popular token pairs - we'll query reserves from the corresponding pools
                # These would be common trading pairs that might show pump potential
                (
                    "WETH",
                    "0xC02aaA39b223FE8D0A0e5C4F27eAD9083C756Cc2",
                    "USDC",
                    "0xA0b86991c6218b36c1d19D4a2e9Eb0cE3606eB48",
                ),
                (
                    "WETH",
                    "0xC02aaA39b223FE8D0A0e5C4F27eAD9083C756Cc2",
                    "USDT",
                    "0xdAC17F958D2ee523a2206206994597C13D831ec7",
                ),
                (
                    "WBTC",
                    "0x2260FAC5E5542a773Aa44fBCfeDf7C193bc2C599",
                    "WETH",
                    "0xC02aaA39b223FE8D0A0e5C4F27eAD9083C756Cc2",
                ),
                (
                    "DAI",
                    "0x6B175474E89094C44Da98b954EedeAC495271d0F",
                    "WETH",
                    "0xC02aaA39b223FE8D0A0e5C4F27eAD9083C756Cc2",
                ),
                (
                    "UNI",
                    "0x1f9840a85d5aF5bf1D1762F925BDADdC4201F984",
                    "WETH",
                    "0xC02aaA39b223FE8D0A0e5C4F27eAD9083C756Cc2",
                ),
            ]

            # Uniswap V2 Pair ABI for getting reserves
            pair_abi = [
                {
                    "constant": True,
                    "inputs": [],
                    "name": "getReserves",
                    "outputs": [
                        {"name": "_reserve0", "type": "uint112"},
                        {"name": "_reserve1", "type": "uint112"},
                        {"name": "_blockTimestampLast", "type": "uint32"},
                    ],
                    "payable": False,
                    "stateMutability": "view",
                    "type": "function",
                },
                {
                    "constant": True,
                    "inputs": [],
                    "name": "token0",
                    "outputs": [{"name": "", "type": "address"}],
                    "payable": False,
                    "stateMutability": "view",
                    "type": "function",
                },
                {
                    "constant": True,
                    "inputs": [],
                    "name": "token1",
                    "outputs": [{"name": "", "type": "address"}],
                    "payable": False,
                    "stateMutability": "view",
                    "type": "function",
                },
            ]

            # Uniswap V2 Factory ABI to find pair addresses
            factory_abi = [
                {
                    "constant": True,
                    "inputs": [
                        {"name": "tokenA", "type": "address"},
                        {"name": "tokenB", "type": "address"},
                    ],
                    "name": "getPair",
                    "outputs": [{"name": "pair", "type": "address"}],
                    "payable": False,
                    "stateMutability": "view",
                    "type": "function",
                }
            ]

            # Uniswap V2 Factory address
            uniswap_factory_address = (
                "0x5C69bEe701ef814a2B6a3EDD4B1652CB9cc5aA6f"  # Uniswap V2 Factory
            )
            factory_contract = self.w3.eth.contract(
                address=uniswap_factory_address, abi=factory_abi
            )

            all_pairs = []

            for token0_symbol, token0_addr, token1_symbol, token1_addr in common_pairs:
                try:
                    # Get the pair address from factory
                    pair_address = factory_contract.functions.getPair(
                        self.w3.to_checksum_address(token0_addr),
                        self.w3.to_checksum_address(token1_addr),
                    ).call()

                    if pair_address != "0x0000000000000000000000000000000000000000":
                        # Create pair contract instance
                        pair_contract = self.w3.eth.contract(
                            address=self.w3.to_checksum_address(pair_address),
                            abi=pair_abi,
                        )

                        # Get reserves
                        reserves = pair_contract.functions.getReserves().call()
                        reserve0 = int(reserves[0])
                        reserve1 = int(reserves[1])

                        # Get actual token addresses from the pair
                        actual_token0 = pair_contract.functions.token0().call()
                        actual_token1 = pair_contract.functions.token1().call()

                        # Calculate price and liquidity based on reserves
                        if reserve0 > 0 and reserve1 > 0:
                            # Price calculation (for a basic example)
                            price_from_reserves = (
                                (reserve1 / 1e18) / (reserve0 / 1e18)
                                if reserve0 > 0
                                else 0
                            )

                            # Estimate liquidity (in USD terms) - simplified calculation
                            # This is an approximation - in production, you'd want to calculate based on actual USD values
                            liquidity_usd = (
                                (reserve0 / 1e18) * 2000
                                if token0_symbol == "WETH"
                                else (reserve1 / 1e18) * 2000
                            )  # Rough USD value

                            pair_info = {
                                "symbol": f"{token0_symbol}/{token1_symbol}",
                                "address": self.w3.to_checksum_address(actual_token0),
                                "pair_address": self.w3.to_checksum_address(
                                    pair_address
                                ),
                                "price_usd": price_from_reserves,
                                "volume_24h": liquidity_usd
                                * 0.1,  # Estimate 10% of liquidity as daily volume
                                "price_change_24h": 0.0,  # Would need historical data
                                "liquidity_usd": liquidity_usd,
                                "txns_24h": {
                                    "buys": 0,
                                    "sells": 0,
                                },  # Would need event logs
                                "dex": "uniswap_v2",
                                "chain": "ethereum",
                                "source": "local_node",
                            }

                            all_pairs.append(pair_info)

                except Exception as e:
                    self.logger.debug(
                        f"Could not fetch local pair for {token0_symbol}-{token1_symbol}: {e}"
                    )
                    continue  # Continue with other pairs

            # Limit the results
            return all_pairs[:limit]

        except Exception as e:
            self.logger.error(f"Error fetching local DEX pairs: {e}")
            return []

    def get_token_history(self, token_address: str, chain: str = "ethereum") -> Dict:
        """
        Get historical price and volume data for a token from local Erigon node,
        with DexScreener as fallback

        Returns data needed for pump detection analysis
        """
        try:
            # First try to get data from local node
            local_history = self.get_token_history_from_local(token_address, chain)
            if local_history and (
                len(local_history["price_data"]) >= 3
                or len(local_history["volume_data"]) >= 3
            ):
                return local_history

            # If local node doesn't have sufficient data, fall back to external API
            self.logger.debug(
                f"Fetching token history from DexScreener for {token_address} (fallback)..."
            )

            url = f"{self.dexscreener_api}/dex/tokens/{token_address}"

            response = requests.get(url, timeout=10)
            response.raise_for_status()
            data = response.json()

            pairs = data.get("pairs", [])
            if not pairs:
                return {"price_data": [], "volume_data": []}

            # Use the highest liquidity pair
            pair = sorted(
                pairs,
                key=lambda x: float(x.get("liquidity", {}).get("usd", 0) or 0),
                reverse=True,
            )[0]

            # Extract time-series data
            price_change_data = pair.get("priceChange", {})
            volume_data_raw = pair.get("volume", {})

            # Build price history from available data points
            current_price = float(pair.get("priceUsd", 0) or 0)
            price_data = []

            # Reconstruct approximate price history from % changes
            for period in ["m5", "h1", "h6", "h24"]:
                change = float(price_change_data.get(period, 0) or 0)
                if change != 0:
                    historical_price = current_price / (1 + change / 100)
                    price_data.append(historical_price)

            price_data.append(current_price)  # Current price last

            # Build volume history (facts only)
            volume_data = []
            for period in ["m5", "h1", "h6", "h24"]:
                volume = float(volume_data_raw.get(period, 0) or 0)
                if volume > 0:
                    volume_data.append(
                        {
                            "volume": volume,
                            "timestamp": datetime.now(timezone.utc).isoformat(),
                            "period": period,
                        }
                    )

            # Facts-only: if insufficient datapoints, return empty series and let callers skip
            return {
                "price_data": price_data if len(price_data) >= 3 else [],
                "volume_data": volume_data if len(volume_data) >= 3 else [],
                "pair_info": pair,
                "source": "dexscreener",  # Indicate this came from external API
            }

        except Exception as e:
            self.logger.error(f"Error fetching token history for {token_address}: {e}")
            return {"price_data": [], "volume_data": []}

    def get_token_history_from_local(
        self, token_address: str, chain: str = "ethereum"
    ) -> Dict:
        """
        Get token history data from local Erigon node by analyzing token contract events
        """
        try:
            if not self.w3 or not self.w3.is_connected():
                return {"price_data": [], "volume_data": []}  # Local node not available

            # For a token address, we need to analyze its trading pair to get price/volume data
            # This is complex as we need to find the liquidity pool where this token trades

            # For now, we'll create a skeleton that queries Uniswap factory for the pair
            # and then gets historical data through the block range
            from web3 import Web3

            # Uniswap V2 Factory address
            uniswap_factory_address = (
                "0x5C69bEe701ef814a2B6a3EDD4B1652CB9cc5aA6f"  # Uniswap V2 Factory
            )

            # ABIs to get the pair and then historical data
            factory_abi = [
                {
                    "constant": True,
                    "inputs": [
                        {"name": "tokenA", "type": "address"},
                        {"name": "tokenB", "type": "address"},
                    ],
                    "name": "getPair",
                    "outputs": [{"name": "pair", "type": "address"}],
                    "payable": False,
                    "stateMutability": "view",
                    "type": "function",
                }
            ]

            # WETH address as a common trading partner
            weth_address = "0xC02aaA39b223FE8D0A0e5C4F27eAD9083C756Cc2"

            factory_contract = self.w3.eth.contract(
                address=uniswap_factory_address, abi=factory_abi
            )

            # Find the pair address for this token + WETH
            pair_address = factory_contract.functions.getPair(
                self.w3.to_checksum_address(token_address),
                self.w3.to_checksum_address(weth_address),
            ).call()

            if pair_address == "0x0000000000000000000000000000000000000000":
                # No pair found with WETH, try with USDC
                usdc_address = "0xA0b86991c6218b36c1d19D4a2e9Eb0cE3606eB48"
                pair_address = factory_contract.functions.getPair(
                    self.w3.to_checksum_address(token_address),
                    self.w3.to_checksum_address(usdc_address),
                ).call()

                if pair_address == "0x0000000000000000000000000000000000000000":
                    # No common pair found, return empty data
                    return {"price_data": [], "volume_data": []}

            # Now that we have a pair, we can get historical data by looking at Swap events
            # This would normally require more complex analysis over a range of blocks
            # For now we'll return skeleton data indicating we have a local source
            return {
                "price_data": [],  # Would need historical Swap events to build proper data
                "volume_data": [],  # Would need Transfer events or Swap events for volume
                "pair_info": {
                    "pair_address": pair_address,
                    "token_address": token_address,
                    "source": "local_node",
                },
                "source": "local_node",
            }

        except Exception as e:
            self.logger.error(
                f"Error fetching local token history for {token_address}: {e}"
            )
            return {"price_data": [], "volume_data": []}

    def calculate_volume_analytics(self, volume_data: List[Dict]) -> Dict:
        """
        Calculate volume-based analytics for pump detection
        """
        try:
            if len(volume_data) < 3:
                return {
                    "volume_spike_ratio": 0.0,
                    "volume_acceleration": 0.0,
                    "volume_concentration": 0.0,
                    "volume_anomaly_score": 0.0,
                    "trend": "STABLE",
                }

            # Calculate recent vs historical volume ratios
            recent_volumes = [v.get("volume", 0) for v in volume_data[-3:]]
            historical_volumes = (
                [v.get("volume", 0) for v in volume_data[:-3]]
                if len(volume_data) > 3
                else recent_volumes
            )

            if not historical_volumes:
                historical_avg = sum(recent_volumes) / len(recent_volumes)
            else:
                historical_avg = sum(historical_volumes) / len(historical_volumes)

            recent_avg = sum(recent_volumes) / len(recent_volumes)
            volume_spike_ratio = (
                recent_avg / historical_avg if historical_avg > 0 else 1.0
            )

            # Calculate volume acceleration (rate of change in volume)
            if len(recent_volumes) >= 2:
                volume_changes = [
                    (recent_volumes[i] - recent_volumes[i - 1]) / recent_volumes[i - 1]
                    if recent_volumes[i - 1] > 0
                    else 0
                    for i in range(1, len(recent_volumes))
                ]
                volume_acceleration = (
                    sum(volume_changes) / len(volume_changes) if volume_changes else 0.0
                )
            else:
                volume_acceleration = 0.0

            # Volume concentration would require address-level data
            volume_concentration = 0.0

            # Determine if it's anomalous
            volume_anomaly_score = 0.0
            if volume_spike_ratio > self.detection_params["volume_spike_threshold"]:
                volume_anomaly_score += min(
                    0.5,
                    (
                        volume_spike_ratio
                        - self.detection_params["volume_spike_threshold"]
                    )
                    * 0.1,
                )

            if (
                volume_acceleration
                > self.detection_params["volume_acceleration_threshold"]
            ):
                volume_anomaly_score += min(0.3, volume_acceleration * 0.1)

            volume_anomaly_score = min(1.0, volume_anomaly_score)

            # Determine trend
            trend = "STABLE"
            if volume_spike_ratio > 5.0:
                trend = "EXPONENTIAL_SPIKE"
            elif volume_spike_ratio > 2.0:
                trend = "HIGH_SPIKE"
            elif volume_spike_ratio > 1.5:
                trend = "MODERATE_SPIKE"

            return {
                "volume_spike_ratio": volume_spike_ratio,
                "volume_acceleration": volume_acceleration,
                "volume_concentration": volume_concentration,
                "volume_anomaly_score": volume_anomaly_score,
                "trend": trend,
                "recent_avg": recent_avg,
                "historical_avg": historical_avg,
            }

        except Exception as e:
            self.logger.error(f"Error calculating volume analytics: {e}")
            return {
                "volume_spike_ratio": 0.0,
                "volume_acceleration": 0.0,
                "volume_concentration": 0.0,
                "volume_anomaly_score": 0.0,
                "trend": "ERROR",
            }

    def calculate_price_momentum(self, price_data: List[float]) -> Dict:
        """
        Calculate price momentum and trend analysis
        """
        try:
            if len(price_data) < 3:
                return {
                    "momentum_score": 0.0,
                    "trend_strength": 0.0,
                    "volatility": 0.0,
                    "acceleration": 0.0,
                    "trend": "STABLE",
                }

            # Calculate price changes
            price_changes = []
            for i in range(1, len(price_data)):
                if price_data[i - 1] != 0:
                    change = (price_data[i] - price_data[i - 1]) / price_data[i - 1]
                    price_changes.append(change)

            if not price_changes:
                return {
                    "momentum_score": 0.0,
                    "trend_strength": 0.0,
                    "volatility": 0.0,
                    "acceleration": 0.0,
                    "trend": "STABLE",
                }

            # Calculate various metrics
            avg_change = sum(price_changes) / len(price_changes)
            volatility = float(np.std(price_changes)) if len(price_changes) > 1 else 0.0
            max_change = max(abs(c) for c in price_changes) if price_changes else 0.0

            # Calculate momentum (recent momentum vs historical)
            recent_changes = (
                price_changes[-3:] if len(price_changes) >= 3 else price_changes
            )
            recent_momentum = (
                sum(recent_changes) / len(recent_changes) if recent_changes else 0.0
            )

            # Acceleration: is the momentum increasing?
            if len(recent_changes) >= 2:
                momentum_changes = [
                    (recent_changes[i] - recent_changes[i - 1])
                    for i in range(1, len(recent_changes))
                ]
                acceleration = (
                    sum(momentum_changes) / len(momentum_changes)
                    if momentum_changes
                    else 0.0
                )
            else:
                acceleration = 0.0

            # Calculate momentum score (higher for rapid, sustained upward movement)
            momentum_score = 0.0
            if (
                recent_momentum > 0.1 and acceleration > 0
            ):  # Positive momentum with acceleration
                momentum_score += min(0.4, recent_momentum * 2)
            if volatility < 0.2 and recent_momentum > 0.05:  # Stable upward trend
                momentum_score += min(0.3, recent_momentum)
            if max_change > 0.2:  # Very large single movement
                momentum_score += min(0.3, max_change - 0.2)

            momentum_score = min(1.0, momentum_score)

            # Determine trend
            trend = "STABLE"
            if recent_momentum > 0.2:
                trend = "RAPID_ASCENSION"
            elif recent_momentum > 0.1:
                trend = "STRONG_UPWARD"
            elif recent_momentum > 0.05:
                trend = "MODERATE_UPWARD"
            elif recent_momentum < -0.2:
                trend = "RAPID_DESCENT"
            elif recent_momentum < -0.1:
                trend = "STRONG_DOWNWARD"

            return {
                "momentum_score": momentum_score,
                "trend_strength": abs(recent_momentum),
                "volatility": volatility,
                "acceleration": acceleration,
                "trend": trend,
                "max_change": max_change,
                "avg_change": avg_change,
            }

        except Exception as e:
            self.logger.error(f"Error calculating price momentum: {e}")
            return {
                "momentum_score": 0.0,
                "trend_strength": 0.0,
                "volatility": 0.0,
                "acceleration": 0.0,
                "trend": "ERROR",
            }

    def correlate_social_volume(
        self, token_symbol: str, time_window_minutes: int = 15
    ) -> Dict:
        """
        Correlate social sentiment with trading volume for pump detection
        """
        try:
            if not self.reddit_monitor:
                return {"sentiment": 0.5, "mentions": 0, "intensity": 0.0}

            # Get social sentiment for the token
            social_data = self.reddit_monitor.get_recent_posts_for_token(
                token_symbol, time_window_minutes
            )

            # Analyze social sentiment
            positive_mentions = sum(
                1 for post in social_data if post.get("sentiment", 0) > 0.5
            )
            negative_mentions = sum(
                1 for post in social_data if post.get("sentiment", 0) < 0.3
            )
            total_mentions = len(social_data)

            # Calculate social metrics
            sentiment_score = 0.5  # Default neutral
            if total_mentions > 0:
                sentiment_score = (
                    positive_mentions / total_mentions if total_mentions > 0 else 0.5
                )

            # Calculate mention intensity (volume of mentions)
            mention_intensity = (
                total_mentions / time_window_minutes if time_window_minutes > 0 else 0
            )

            return {
                "sentiment": sentiment_score,
                "mentions": total_mentions,
                "intensity": mention_intensity,
            }

        except Exception as e:
            self.logger.error(f"Error correlating social and volume data: {e}")
            return {"sentiment": 0.5, "mentions": 0, "intensity": 0.0}

    def detect_pump_scheme(
        self,
        token_symbol: str,
        token_address: str,
        price_data: List[float],
        volume_data: List[Dict],
    ) -> Dict:
        """
        Main function to detect pump schemes using REAL data
        """
        try:
            self.logger.info(f"Analyzing pump scheme for token: {token_symbol}")

            # Calculate volume analytics
            volume_analysis = self.calculate_volume_analytics(volume_data)

            # Calculate price momentum
            price_analysis = self.calculate_price_momentum(price_data)

            # Correlate social and volume data
            social_signals = self.correlate_social_volume(token_symbol)

            # Calculate overall pump score
            pump_score = (
                volume_analysis.get("volume_anomaly_score", 0) * 0.4
                + price_analysis.get("momentum_score", 0) * 0.35
                + (social_signals.get("intensity", 0) / 10)
                * 0.25  # Normalize intensity
            )

            # Additional factors
            if volume_analysis.get("volume_spike_ratio", 0) > 10.0:
                pump_score += 0.2  # Massive volume spike
            elif volume_analysis.get("volume_spike_ratio", 0) > 5.0:
                pump_score += 0.1  # Large volume spike

            if price_analysis.get("max_change", 0) > 0.5:
                pump_score += 0.15  # Very large single price movement

            # Apply social media correlation if strong
            if (
                social_signals.get("intensity", 0) > 10.0
                and social_signals.get("sentiment", 0.5) > 0.7
            ):
                pump_score += 0.1  # High positive mentions

            pump_score = min(1.0, pump_score)  # Cap at 1.0

            # Identify specific patterns detected
            detected_patterns = []
            if (
                volume_analysis.get("volume_spike_ratio", 0)
                > self.detection_params["volume_spike_threshold"]
            ):
                detected_patterns.append("VOLUME_SPIKE")
            if (
                price_analysis.get("momentum_score", 0)
                > self.detection_params["price_momentum_threshold"]
            ):
                detected_patterns.append("HIGH_MOMENTUM")
            if social_signals.get("intensity", 0) > 5.0:
                detected_patterns.append("HIGH_SOCIAL_MENTIONS")

            # Determine risk level
            risk_level = "LOW"
            if pump_score > 0.7:
                risk_level = "CRITICAL"
            elif pump_score > 0.4:
                risk_level = "HIGH"
            elif pump_score > 0.2:
                risk_level = "MEDIUM"

            # Store analysis in database
            self._store_analysis(
                token_address=token_address,
                token_symbol=token_symbol,
                pump_score=pump_score,
                risk_level=risk_level,
                detected_patterns=str(detected_patterns),
                volume_spike_ratio=volume_analysis.get("volume_spike_ratio", 0),
                price_momentum=price_analysis.get("momentum_score", 0),
                social_signals=str(social_signals),
            )

            # Prepare comprehensive result
            result = {
                "token_symbol": token_symbol,
                "token_address": token_address,
                "timestamp": datetime.now(timezone.utc).isoformat(),
                "pump_score": round(pump_score, 3),
                "risk_level": risk_level,
                "detected_patterns": detected_patterns,
                "volume_spike_ratio": volume_analysis.get("volume_spike_ratio", 0),
                "price_momentum": price_analysis.get("momentum_score", 0),
                "social_signals": social_signals,
            }

            # Publish significant findings to Redis
            if pump_score > 0.15:
                payload = {
                    "type": "PUMP_DETECTED",
                    "service": "pump_detector",
                    "message": f"Pump risk {risk_level} {token_symbol} (score {result['pump_score']})",
                    "timestamp": result["timestamp"],
                    "token_symbol": token_symbol,
                    "token_address": token_address,
                    "pump_score": str(result["pump_score"]),
                    "risk_level": result["risk_level"],
                    "data": json.dumps(result),
                }
                self.redis_client.xadd(
                    "signals.pump_detector",
                    payload,
                    maxlen=20000,
                    approximate=True,
                )
                self.logger.info(
                    f"üö® Pump detected: {token_symbol} (score: {pump_score:.3f})"
                )

            return result

        except Exception as e:
            self.logger.error(f"Error detecting pump scheme for {token_symbol}: {e}")
            return {
                "token_symbol": token_symbol,
                "token_address": token_address,
                "error": str(e),
                "pump_score": 0,
                "risk_level": "ERROR",
            }

    def _store_analysis(
        self,
        token_address: str,
        token_symbol: str,
        pump_score: float,
        risk_level: str,
        detected_patterns: str,
        volume_spike_ratio: float,
        price_momentum: float,
        social_signals: str,
    ):
        """Store pump detection analysis in database"""
        for attempt in range(3):
            try:
                conn = connect_db(self.db_path, timeout=120)
                cursor = conn.cursor()

                cursor.execute(
                    """
                    INSERT INTO pump_detection
                    (token_address, token_symbol, pump_score, risk_level,
                     detected_patterns, volume_spike_ratio, price_momentum, social_signals)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?)
                """,
                    (
                        token_address,
                        token_symbol,
                        pump_score,
                        risk_level,
                        detected_patterns,
                        volume_spike_ratio,
                        price_momentum,
                        social_signals,
                    ),
                )

                conn.commit()
                conn.close()
                return  # Success
            except sqlite3.OperationalError as e:
                if "database is locked" in str(e).lower() and attempt < 2:
                    # Backoff with jitter
                    time.sleep(0.25 + (attempt * 0.25))
                    continue
                self.logger.error(f"Error storing pump detection (after retries): {e}")
                break
            except Exception as e:
                self.logger.error(f"Error storing pump detection: {e}")
                break

    def monitor_token_pump_risk(self, token_info: Dict) -> Optional[Dict]:
        """
        Monitor a specific token for pump risk using REAL data
        """
        try:
            token_symbol = token_info.get("symbol", "")
            token_address = token_info.get("address", "")

            self.logger.info(
                f"Monitoring pump risk for token: {token_symbol} ({token_address})"
            )

            # Get REAL historical data
            history = self.get_token_history(token_address)
            price_data = history.get("price_data", [])
            volume_data = history.get("volume_data", [])

            if not price_data or not volume_data:
                self.logger.warning(f"No data available for {token_symbol}")
                return None

            # Perform pump detection analysis
            return self.detect_pump_scheme(
                token_symbol, token_address, price_data, volume_data
            )

        except Exception as e:
            self.logger.error(f"Error monitoring pump risk for {token_symbol}: {e}")
            return None

    async def run_pump_monitoring(self):
        """Run continuous pump detection monitoring using REAL DATA"""
        self.logger.info("üöÄ Starting pump detection monitoring loop (REAL DATA MODE)")

        while True:
            try:
                # Get REAL trending tokens from CoinGecko
                trending_tokens = self.get_trending_tokens(limit=10)

                # Get REAL DEX pairs from DexScreener
                dex_pairs = self.get_dexscreener_pairs(limit=10)

                # Combine both sources
                all_tokens = trending_tokens + dex_pairs

                self.logger.info(
                    f"Analyzing {len(all_tokens)} tokens for pump schemes..."
                )

                for token_info in all_tokens:
                    try:
                        result = self.monitor_token_pump_risk(token_info)

                        if result and result.get("pump_score", 0) > 0.4:
                            self.logger.warning(
                                f"‚ö†Ô∏è High pump risk: {result['token_symbol']} "
                                f"(score: {result['pump_score']:.3f}, risk: {result['risk_level']})"
                            )

                        # Rate limiting - don't hammer APIs
                        await asyncio.sleep(2)

                    except Exception as e:
                        self.logger.error(
                            f"Error analyzing token {token_info.get('symbol')}: {e}"
                        )
                        continue

                # Wait before next monitoring cycle (10 minutes)
                self.logger.info("‚úÖ Monitoring cycle complete. Waiting 10 minutes...")
                await asyncio.sleep(600)

            except Exception as e:
                self.logger.error(f"Error in pump monitoring loop: {e}")
                await asyncio.sleep(60)


def main():
    """Main function to run the Pump Detector"""
    detector = PumpDetector()

    # Health mode
    if len(sys.argv) > 1 and sys.argv[1] == "--health":
        print(json.dumps(detector.health(), indent=2))
        return

    if len(sys.argv) > 1:
        # Manual token analysis mode
        token_symbol = sys.argv[1]
        token_address = sys.argv[2] if len(sys.argv) > 2 else ""

        token_info = {"symbol": token_symbol, "address": token_address}

        result = detector.monitor_token_pump_risk(token_info)
        if result:
            print(json.dumps(result, indent=2))
    else:
        # Continuous monitoring mode
        try:
            detector.logger.info("üöÄ Starting Pump Detector continuous monitoring")
            asyncio.run(detector.run_pump_monitoring())
        except KeyboardInterrupt:
            detector.logger.info("Pump Detector stopped by user")
        except Exception as e:
            detector.logger.error(f"Pump Detector failed: {e}")
            raise


if __name__ == "__main__":
    main()
